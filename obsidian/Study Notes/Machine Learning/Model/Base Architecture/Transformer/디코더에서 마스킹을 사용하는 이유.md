---
title: 디코더에서 마스킹을 사용하는 이유
date: 2025-12-01
categories: []
tags:
  - Transformer
keywords:
  - LLM
  - Transformer
---
마스킹의 경우, 학습 단계를 위한 마스킹임
# 1. 학습 단계에서의 디코더의 Masked Attention 과정
---
병렬 처리를 통해 학습하기 위해, 여러 개의 토큰을 한번에 Query로 입력
- 입력: \[SOS], \[나는], \[학교에]
- Query (Q): 3개의 행
- Key (K): 3개의 열

| Query\| Key | [SOS] | “나는”   | “학교에”  | 예측목표 |
| ----------- | ----- | ------ | ------ | ---- |
| Q1: [SOS]   | V11   | [mask] | [mask] | 나는   |
| Q2: “나는”    | V21   | V22    | [mask] | 학교에  |
| Q3: “학교에”   | V31   | V32    | V33    | 간다   |
- 특징:
	1. Q가 3줄이며, 한번에 3문제를 동시에 푸는 것임
	2. 예측해야할 다음 시점을 미리 보면 안되므로, [mask] 토큰을 활용
	3. Q1, Q2, Q3의 결과를 동시에 계산해서 오차를 역전파


# 2. 추론 과정 예시 (KV Cache 사용 X)
---
시나리오: [SOS] → “나는” → “학교에” 예측
- STEP 1: 첫 번째 단어 생성
	- 입력: [SOS]
	- Query: [SOS] (1행)
	- Key: [SOS] (1열)
		- | Q \| K    | [SOS] |
| --------- | ----- |
| Q1: [SOS] | V11   |
<br>
- STEP2: 두 번째 단어 생성 (Non-Caching)
	- KV Cache가 없으므로, STEP1에서 계산했던 [SOS]의 정보를 기억하지 못하며, 다시 처음부터 계산
	- 입력: [SOS], [나는]
	- Query: [SOS], [나는]
	- Key: [SOS], [나는]
	- 어텐션 행렬
		- |Q \ K|[SOS]|나는|
|---|---|---|
|Row 1 ([SOS])|또 계산함 (낭비)| [MASK]|
|Row 2 (나는)|(New)|(New)|

	- Row1 ([SOS]): 자기 자신을 보고 다음 단어를 예측 -> [나는], 그렇지만 이건 이미 STEP1에서 예측한 값과 동일함 (낭비)
	- Row2 ([나는]): 앞의 [SOS] 및 자기자신[나는] 을 이용해 다음 단어 예측
	- 출력: [학교에] 예측
	<br>
- STEP 3: 세번째 단어 생성
	- 처음부터 현재까지 예측한 모든 토큰을 다시 입력
	- 입력: [SOS], [나는], [학교에]
	- Query: [SOS], [나는], [학교에]
	- 어텐션 행렬
		- |Q \ K|[SOS]|나는|학교에|
|---|---|---|---|
|Row 1 ([SOS])|또 계산 (낭비)|[Mask]|[Mask]|
|Row 2 (나는)|또 계산 (낭비)|또 계산 (낭비)|[Mask]|
|Row 3 (학교에)|(New)|(New)|(New)|


# 3. 모델 자체 결함: 추론과정에서도 [mask] 토큰 생성
- [SOS] 토큰 이후, [EOS] 토큰이 나올 때 까지, 연속적인 추론을 하면서 다음 토큰을 생성하게 되며, 그 과정에서 mask 토큰 생성 함수도 함께 작동함. 따라서 추론 과정에서도 어텐션 스코어 행렬에 이전 단계에서 추론한 단어토큰 및 [mask] 토큰이 함께 포함됨
- 다음 토큰을 예측하기 위해, KV Cache를 사용하지 않을 경우, 이전 토큰들의 가중치를 활용하기 위해서는 또 다시 계산해야함
	- 예) [SOS] → [나는] → 예측 [학교에]
		- [학교에]를 예측하기 위해 [SOS]와 [나는]의 정보 (key, value)를 확인해야함
		- 따라서 다시 입력단에 [SOS]와 [나는]을 집어넣음
		- 다시 어텐션 행렬을 구성하게 되며, 학습에서 사용하던 함수들을 그대로 활용하므로 [mask] 토큰이 포함된 행렬을 생성
		- 그러나 어텐션 행렬 Row1:[SOS], Row2: [SOS], [나는], Row3: [SOS], [나는], [학교에] 중에서 마지막 행, Row3만 필요함  











# 기타)
인코더에서 사용하는 것은 패딩 토큰임
	- 여러 문장을 한번에 처리하기 위해서는 문장의 길이를 똑같이 맞춰줘야하고, 이를 위해서 패딩 토큰을 사용함
	- 패딩 토큰은 0 or \[pad]를 사용

---
- Query (질문할 대상): 디코더의 입력 (t-1 시점)
- Key (참조할 정보): 디코더의 입력 (t 시점까지)

입력 문장: “오늘 어디에 갈꺼니?”
최종 출력 문장: “나는 학교에 간다.”

- STEP1: 입력문장 + “나는”
	- 입력문장을 제외해본다면, “나는” x “나는”의 스코어를 계산 후
	- 다음 단어인 “학교에”를 예측

- STEP2: “나는” + “학교에”를 입력
	- 보통 Query가 행이고, Key가 열이 됨
	- 시점(t_k)별 Query 및 Key가 행렬로 들어가게 되어, 어텐션 스코어를 계산
	- 첫번째 행(step1)에 대한 Key는 “나는” + \[Masked]의 형태로 어텐션 행렬계산에 이용
	- 두번째 행에 대한 Key는 “나는” + “학교에” 이며, 이는 query 열과 동일
	- 어텐션 스코어 계산 후, “간다”라는 단어를 예측
	- \[Masked] 토큰을 사용하는 이유는, 모델이 예측한 시점에 대한 흐름을 알도록 하기 위해서임 (미래 참조 방지)

|                  |              |                    |                                                       |
| ---------------- | ------------ | ------------------ | ----------------------------------------------------- |
| Query\| Key      | "나는" (Col 1) | "학교에" (Col 2)      | 설명                                                    |
| Row 1 <br>("나는") | ✅ 볼 수 있음     | 🚫 **Masked** (-∞) | 1번 시점("나는")은 미래("학교에")를 보면 안 됨                        |
| Row 2<br>("학교에") | ✅ 볼 수 있음     | ✅ 볼 수 있음           | 2번 시점("학교에")에서는 과거("나는")와 현재("학교에")를 모두 참고해서 "간다"를 예측 |
|                  |              |                    |                                                       |
|                  |              |                    |                                                       