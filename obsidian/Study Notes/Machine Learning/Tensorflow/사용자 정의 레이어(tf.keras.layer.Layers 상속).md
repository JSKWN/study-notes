`tf.keras.layers.Layer`ë¥¼ ìƒì†ë°›ì•„ ì‚¬ìš©ì ì •ì˜ ë ˆì´ì–´ë¥¼ ë§Œë“¤ ë•Œ, Tensorflowì˜ â€˜Graph Execution*â€™ê³¼ â€˜Variable Management*â€™ ë©”ì»¤ë‹ˆì¦˜ì„ ìœ„í•´ ë°˜ë“œì‹œ êµ¬í˜„í•´ì•¼í•˜ëŠ” ë©”ì„œë“œ ì¡´ì¬. (ê·¸ë˜í”„ ì‹¤í–‰ì€ ì†ë„, ì„±ëŠ¥ì˜ ì´ì )

### 0.Â __init__(self, **kwargs)
- **ì—­í• :**Â ê°ì²´ ìƒì„± ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° ì´ˆê¸°í™”.
- **í•„ìˆ˜:**Â super().__init__(**kwargs)Â í˜¸ì¶œ (ë ˆì´ì–´ ì´ë¦„, dtype ë“±ì˜ ì²˜ë¦¬ë¥¼ ìœ„í•¨).
- **êµ¬í˜„:**
    - í•„í„° ìˆ˜, ì»¤ë„ í¬ê¸° ë“± ë ˆì´ì–´ ë™ì‘ì— í•„ìš”í•œ ì„¤ì • ê°’ì„ ë©¤ë²„ ë³€ìˆ˜ë¡œ ì €ì¥ (self.filters = filters).
    - ë‚´ë¶€ì—ì„œ ì‚¬ìš©í•  í•˜ìœ„ ë ˆì´ì–´(Conv2D,Â DenseÂ ë“±)ë¥¼ ì •ì˜ (self.dense = layers.Dense(32)).
### 1.Â build(self, input_shape)
- **ì—­í• :**Â ì…ë ¥ í…ì„œì˜ í˜•íƒœ(input_shape)ê°€ í™•ì •ë˜ëŠ” ì‹œì ì— í˜¸ì¶œë˜ì–´, **ê°€ì¤‘ì¹˜(Weight)**ì™€ **í¸í–¥(Bias)**ì„ ìƒì„±(ì§€ì—° ì´ˆê¸°í™”).
- **êµ¬í˜„:**
    - **self.add_weight()Â ì‚¬ìš©:**Â ì§ì ‘Â tf.Variableì„ ì“°ì§€ ì•Šê³  ì´ ë©”ì„œë“œë¥¼ ì¨ì•¼ Kerasê°€ ê°€ì¤‘ì¹˜ë¥¼ ì¶”ì (Trainable/Non-trainable)í•˜ê³  ë°±ì—”ë“œì— ë“±ë¡í•  ìˆ˜ ìˆìŒ.
    - **Shape ê²°ì •:**Â input_shapeë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°€ì¤‘ì¹˜ì˜ í¬ê¸°ë¥¼ ê³„ì‚°í•˜ì—¬ ì„¤ì •.
- **ì£¼ì˜:**Â ë°˜ë“œì‹œ`super().build(input_shape)`ë¥¼ í˜¸ì¶œí•˜ì—¬ ë ˆì´ì–´ ë¹Œë“œ ì™„ë£Œ ìƒíƒœ(self.built = True)ë¡œ ë§Œë“¤ì–´ì•¼ í•¨.

### 2.Â call(self, inputs, training=None)
- **ì—­í• :**Â ìˆœë°©í–¥ ì „íŒŒ(Forward Pass) ë¡œì§ êµ¬í˜„.
- **ì£¼ìš” ì¸ì:**
    - inputs: ì…ë ¥ í…ì„œ.
    - training: í•™ìŠµ(True)ê³¼ ì¶”ë¡ (False) ì‹œ ë™ì‘ì´ ë‹¤ë¥¸ ê²½ìš°(ì˜ˆ: Dropout, BatchNormalization) ë¶„ê¸° ì²˜ë¦¬ì— ì‚¬ìš©.
- **ì£¼ì˜:**Â ì—°ì‚° ì‹œÂ numpyÂ í•¨ìˆ˜ ëŒ€ì‹  TensorFlow Ops (tf.matmul,Â tf.nn.reluÂ ë“±)ë¥¼ ì‚¬ìš©í•´ì•¼ ê·¸ë˜í”„ ëª¨ë“œ ìµœì í™”ê°€ ê°€ëŠ¥í•¨.

### 3.Â get_config(self)
- **ì—­í• :**Â ëª¨ë¸ ì €ì¥/ë¡œë“œ ì‹œ ë ˆì´ì–´ì˜ ì„¤ì •ì„ ìœ ì§€(ì§ë ¬í™”)í•˜ê¸° ìœ„í•¨.
- **êµ¬í˜„:**
    - config = super().get_config()ë¡œ ë¶€ëª¨ ì„¤ì • ë¡œë“œ.
    - init__ì—ì„œ ë°›ì€ ì¸ìë“¤ì„Â config.update({...})ë¡œ ì¶”ê°€í•˜ì—¬ ë°˜í™˜.


#### ì°¸ê³  ì†ŒìŠ¤ì½”ë“œ
```Python
import tensorflow as tf
from tensorflow.keras import layers

class MyCustomLayer(layers.Layer):
    def __init__(self, output_dim, **kwargs):
        super(MyCustomLayer, self).__init__(**kwargs)
        self.output_dim = output_dim
        # ì„œë¸Œ ë ˆì´ì–´ê°€ í•„ìš”í•˜ë‹¤ë©´ ì—¬ê¸°ì„œ ì •ì˜
        # self.dense = layers.Dense(output_dim) 

    def build(self, input_shape):
        # input_shape: (batch_size, input_dim)
        # ê°€ì¤‘ì¹˜ ìƒì„± (Lazy Initialization)
        self.w = self.add_weight(
		        # shape: ë‚´ê°€ ì •ì˜í•œ ë ˆì´ì–´ë¡œ ë“¤ì–´ì˜¤ê³  ë‚˜ê°€ëŠ” í…ì„œì˜ ì°¨ì›ìˆ˜
            shape=(input_shape[-1], self.output_dim),
            initializer="random_normal",
            trainable=True,
            name="kernel"
        )
        self.b = self.add_weight(
            shape=(self.output_dim,),
            initializer="zeros",
            trainable=True,
            name="bias"
        )
        # ì¤‘ìš”: ëì— ìƒìœ„ í´ë˜ìŠ¤ì˜ build í˜¸ì¶œ
        super(MyCustomLayer, self).build(input_shape)

    def call(self, inputs, training=None):
        # ìˆœë°©í–¥ ì—°ì‚° ì •ì˜
        output = tf.matmul(inputs, self.w) + self.b
        
        # training ì¸ìì— ë”°ë¥¸ ë¶„ê¸° ì²˜ë¦¬ ì˜ˆì‹œ
        # if training:
        #     output = tf.nn.dropout(output, rate=0.5)
        
        return output

    def get_config(self):
        config = super(MyCustomLayer, self).get_config()
        config.update({
            "output_dim": self.output_dim
        })
        return config

```



(ì°¸ê³ )
- ğŸ”§ Graph Execution, Eager Execution
	- ì‚¬ìš© ì‹œ
		- ê°œë°œ ë‹¨ê³„(ëª¨ë¸ êµ¬í˜„ ë° ë””ë²„ê¹…): ì¦‰ì‹œ ì‹¤í–‰(Eager; ê¸°ë³¸ì„¤ì •)ì„ ì´ìš©í•˜ì—¬ ê°œë°œ
		- í•™ìŠµ ë‹¨ê³„: í•œ ìŠ¤í…ë‹¹ í›ˆë ¨ ë¡œì§ í•¨ìˆ˜ì— `@tf.function`ë°ì½”ë ˆì´í„°ë¥¼ ì‚¬ìš©
			```Python
				@tf.function 
				def train_step(images, labels): with tf.GradientTape() as tape:
					predictions = model(images, training=True)
					loss = loss_object(labels, predictions)
					gradients = tape.gradient(loss, model.trainable_variables)
					optimizer.apply_gradients(zip(gradients, model.trainable_variables))
					return loss
			```
	- **ì¦‰ì‹œ ì‹¤í–‰ (Eager):**
		- MatMulÂ â†’Â AddÂ â†’Â ReLUÂ ì—°ì‚°ì„ ìˆ˜í–‰í•  ë•Œ, ê°ê°ì˜ ì—°ì‚°ë§ˆë‹¤ GPU ì»¤ë„(Kernel)ì„ í˜¸ì¶œí•˜ê³  ë©”ëª¨ë¦¬ì— ì“°ê³  ì½ëŠ” ê³¼ì •ì„ ë°˜ë³µí•˜ë©°, ì´ëŠ” ë©”ëª¨ë¦¬ ëŒ€ì—­í­(Memory Bandwidth)ì´ ë‚­ë¹„ë˜ëŠ” ìš”ì¸
		- ì—°ì‚° í•˜ë‚˜ê°€ ëë‚  ë•Œë§ˆë‹¤ ì œì–´ê¶Œì´ Python ì¸í„°í”„ë¦¬í„°ë¡œ ëŒì•„ì˜´. Pythonì€ ì¸í„°í”„ë¦¬í„° ì–¸ì–´ íŠ¹ì„±ìƒ ì†ë„ê°€ ëŠë¦¬ê³ , GIL(Global Interpreter Lock)ë¡œ ì¸í•´ ë©€í‹°ìŠ¤ë ˆë”© íš¨ìœ¨ì´ ë–¨ì–´ì§. GPUëŠ” ì—°ì‚°ì´ ë§¤ìš° ë¹ ë¥¸ë°, Pythonì´ ë‹¤ìŒ ëª…ë ¹ì„ ì¤„ ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¬ëŠ” GPU ìœ íœ´ ì‹œê°„(Idle time)ì´ ë°œìƒ
	- **ê·¸ë˜í”„ ì‹¤í–‰ (Graph):**Â 
		- ì»´íŒŒì¼ëŸ¬ê°€ ì „ì²´ ê·¸ë˜í”„ë¥¼ ë¶„ì„í•˜ì—¬, ê°€ëŠ¥í•œ ì—°ì‚°ë“¤ì„ í•˜ë‚˜ë¡œ í•©ì¹¨. ì˜ˆë¥¼ ë“¤ì–´Â MatMul + Add + ReLUë¥¼ í•˜ë‚˜ì˜ ì»¤ë„ë¡œ ìœµí•©(Fusion)í•˜ì—¬ ì‹¤í–‰
		- ê·¸ë˜í”„ê°€ ìƒì„±ë˜ë©´ ì „ì²´ ê³„ì‚° ë¡œì§ì´ C++ ëŸ°íƒ€ì„ìœ¼ë¡œ ì „ë‹¬ë¨. Python ì¸í„°í”„ë¦¬í„°ë¥¼ ê±°ì¹˜ì§€ ì•Šê³  C++ ë ˆë²¨ì—ì„œ ì—°ì†ì ìœ¼ë¡œ ì—°ì‚°ì„ ìˆ˜í–‰í•˜ë¯€ë¡œ, GPUì— ëŠê¹€ ì—†ì´ ëª…ë ¹ì„ ê³µê¸‰í•  ìˆ˜ ìˆìŒ.
		- https://www.tensorflow.org/guide/intro_to_graphs?hl=ko
	- â­`tf.saved_model.save()` â†’ ê·¸ë˜í”„ í˜•íƒœë¡œ ë³€í™˜ í›„ ì €ì¥
	

- `tf.Variable`: ê°€ì¤‘ì¹˜ì— ëŒ€í•œ ìƒíƒœë¥¼ ì €ì¥í•˜ëŠ” ê°ì²´
	- Mutable: í…ì„œëŠ” ê¸°ë³¸ì ìœ¼ë¡œ immutableì´ì§€ë§Œ, Variableì€ ë‚´ë¶€ì˜ ê°’ì´ ë³€ê²½ ê°€ëŠ¥
	- Device Placement: `tf.Variable`ì€ ì„ ì–¸ë˜ëŠ” ìˆœê°„, GPUë©”ëª¨ë¦¬(VRAM)ì— í• ë‹¹ë  ìˆ˜ ìˆìŒ
	- kerasì˜ `Layer` ë‚˜ `Model` í´ë˜ìŠ¤ëŠ” ì´ëŸ¬í•œ ë³€ìˆ˜ ì¶”ì ì„ ìë™ìœ¼ë¡œ ìˆ˜í–‰
	-  build()Â ë©”ì„œë“œì™€ Variable Management:
	    - ì…ë ¥ ë°ì´í„°ì˜ í¬ê¸°ê°€ í™•ì •ëœ ì‹œì ì—Â tf.Variableì„ ìƒì„±í•˜ì—¬Â GPU ë©”ëª¨ë¦¬ì— ê³µê°„ì„ í• ë‹¹í•˜ê³ , ë ˆì´ì–´ê°€ ì´ ë³€ìˆ˜ë¥¼Â ì¶”ì í•˜ë„ë¡ ë“±ë¡í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.