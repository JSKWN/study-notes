
- [[#1. 연구 및 개발 단계 (일반적)|1. 연구 및 개발 단계 (일반적)]]
- [[#2. 실제 서비스/배포 단계 (Production)|2. 실제 서비스/배포 단계 (Production)]]
- [[#(참고) 학습 및 저장 단계에서의 Eager/Graph Execution|(참고) 학습 및 저장 단계에서의 Eager/Graph Execution]]

```Python
import tensorflow as tf
from tensorflow.keras import layers

# [필수] 클래스 소스 코드가 이 파일에 있거나, 다른 모듈에서 import 해와야 함
class ResidualBlock3D(layers.Layer):
    def __init__(self, num_filters, kernel_size=(3,3,3), **kwargs):
        # ... (생략) ...
    
    def call(self, inputs):
        # ... (이 계산 로직이 필요함) ...
        return x
    
    def get_config(self):
        # ... (생략) ...

# [복구] 소스 코드가 메모리에 있는 상태에서 로드 진행
model = tf.keras.models.load_model(
    "my_model.h5", 
    custom_objects={"ResidualBlock3D": ResidualBlock3D} # 클래스 자체를 넘겨줌
)
```

### 1. 연구 및 개발 단계 (일반적)
- 방법: 소스 코드 + `custom_objects` 사용
- 모델을 불러와서 추가 학습(Fine-tuning)을 하거나, 모델 구조를 디버깅해야 할 때 사용
- `tf.keras.models.load_model`의 인자로 `custom_object` 및 클래스 소스코드를 매번 주는 대신, 클래스 정의 위에 데코레이터를 붙여 자동으로 등록할 수 있음.
    ```python
    @tf.keras.utils.register_keras_serializable()
    class ResidualBlock3D(layers.Layer):
        ...
    # 이렇게 하면 load_model을 사용할때 자동으로 클래스를 찾음
    ```

### 2. 실제 서비스/배포 단계 (Production)
- 방법: SavedModel 포맷 또는 ONNX / TFLite 변환
- 배포 환경(서버, 모바일 등)에 파이썬 소스 코드를 포함시키는 것은 비효율적이며 보안 및 관리 이슈가 발생함. 따라서 연산 로직인 "그래프(Graph)" 자체를 Freezing하는 방식을 사용.
    * TensorFlow Serving (SavedModel):
	    * .h5 파일 대신 SavedModel 폴더 형식으로 저장하여 모델의 연산 그래프를 고정함.
	    *  C++ 기반의 TF Serving 서버에 올리면 파이썬 코드 없이도 고성능 추론(Inference)이 가능.
    *   ONNX / TFLite:
	    * 모델을 범용 포맷(ONNX)이나 모바일용(TFLite)으로 변환
	    *  `.onnx` 파일만 있으면 C#, Java, C++ 등 다양한 언어와 환경에서 실행할 수 있음.


### (참고) 학습 및 저장 단계에서의 Eager/Graph Execution
1. Case A: 즉시 실행으로 학습하고 → 즉시 실행으로 추론
	- 개발자가 GradientTape을 써서 순수 Python 코드로 학습시키고, 추론도 Python 스크립트에서 그대로 돌리는 경우.
	- 연산 그래프 최적화가 이루어지지 않아 속도가 느림 (최적화 안 됨).
2. Case B: 그래프 실행으로 학습하고 → 그래프 실행으로 추론
	- @tf.function을 붙여서 학습시키고, SavedModel로 저장해서 서빙하는 경우.
	- 전 과정이 그래프로 처리되어 빠름 (최적화 됨).
3. Case C: 즉시 실행으로 학습하고 → 그래프 실행으로 추론 (가장 권장됨)
	- 학습 시에는 **즉시 실행(Eager Execution)** 모드를 사용. 학습을 마친 뒤 모델을 저장할 때 tf.saved_model.save() 등을 통해 내부 연산을 **그래프 형태(Tracing)로 변환**하여 저장.
	- 추론 시에는 Case B와 동일한 성능을 냄 (**최적화 됨**).