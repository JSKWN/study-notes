

# 1. 디코더 작동 방식: K,V 캐시 없는 경우
- KV 캐시가 없으면 지금까지 쓴 문장을 매번 처음부터 다시 읽어야 함
	- ‘지금까지 생성된 모든 토큰’을 한꺼번에 입력으로 넣음 






# 2. 디코더 작동 방식: K,V 캐시 사용






# (25-12-01) 임시, LLM 대화이력 우선 첨부
나중에 자료 찾아서 보충 후 정리할 것
- [[#1. KV 캐시가 왜 필요한가? (기본 디코더의 비효율성)|1. KV 캐시가 왜 필요한가? (기본 디코더의 비효율성)]]
	- [[#1. KV 캐시가 왜 필요한가? (기본 디코더의 비효율성)#KV 캐시가 없을 때 (매 스텝마다 일어나는 일)|KV 캐시가 없을 때 (매 스텝마다 일어나는 일)]]
	- [[#1. KV 캐시가 왜 필요한가? (기본 디코더의 비효율성)#KV 캐시가 있을 때|KV 캐시가 있을 때]]
- [[#2. Speculative Decoding에서 KV 캐시가 중요한 이유|2. Speculative Decoding에서 KV 캐시가 중요한 이유]]
	- [[#2. Speculative Decoding에서 KV 캐시가 중요한 이유#이때 "병렬 검증" 단계에서 일어나는 일|이때 "병렬 검증" 단계에서 일어나는 일]]
- [[#3. 요약|3. 요약]]
- [[#기본 원칙: "전체 입력, 전체 재계산"|기본 원칙: "전체 입력, 전체 재계산"]]
- [[#Step 1. 첫 시작 ("나는" 입력)|Step 1. 첫 시작 ("나는" 입력)]]
- [[#Step 2. 두 번째 스텝 (중요: 여기서부터 낭비 시작)|Step 2. 두 번째 스텝 (중요: 여기서부터 낭비 시작)]]
- [[#Step 3. 세 번째 스텝 (낭비 심화)|Step 3. 세 번째 스텝 (낭비 심화)]]
- [[#핵심 비교: 캐시가 있고 없고의 차이|핵심 비교: 캐시가 있고 없고의 차이]]

speculative decoding을 보고있는데,
드래프트 생성 -> 병렬 검증 -> 거절 샘플링 단계 중에
병렬 검증 단계에서 KV 캐시가 중요하다고 하는데, KV 캐시가 뭔지 모르겠어

----

**KV 캐시(Key-Value Cache)**는 트랜스포머 모델이 텍스트를 생성할 때 **"이미 계산했던 과거의 정보들을 메모리에 저장해두고 재사용하는 기술"**입니다.

님께서 트랜스포머 디코더 구조를 아시니, **Self-Attention** 메커니즘을 통해 설명하면 아주 쉽습니다.

---

### 1. KV 캐시가 왜 필요한가? (기본 디코더의 비효율성)

트랜스포머 디코더에서 $t$번째 토큰을 생성하려면, **현재 토큰($t$)이 과거의 모든 토큰($0 \sim t-1$)과 어떤 관계가 있는지(Attention)**를 계산해야 합니다.

Self-Attention 수식 기억나시죠?
$$Attention(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V$$

*   **Q (Query):** "지금 내가 궁금한 것" (현재 생성할 토큰의 관점)
*   **K (Key):** "과거 토큰들의 인덱스/특징" (검색 대상)
*   **V (Value):** "과거 토큰들의 실제 내용 값"

#### KV 캐시가 없을 때 (매 스텝마다 일어나는 일)
1.  **Step 1:** "나는" 생성 $\rightarrow$ "나는"의 $K, V$ 계산.
2.  **Step 2:** "나는" + "학교에" 생성 $\rightarrow$ **"나는"의 $K, V$ 다시 계산** + "학교에"의 $K, V$ 계산.
3.  **Step 3:** "나는" + "학교에" + "간다" 생성 $\rightarrow$ **"나는", "학교에" $K, V$ 또 다시 계산** + "간다" $K, V$ 계산.

$\rightarrow$ 과거 토큰의 $K$와 $V$ 값은 변하지 않는데, 매번 처음부터 다시 계산하는 엄청난 낭비가 발생합니다.

#### KV 캐시가 있을 때
*   과거 토큰들의 $K, V$ 벡터를 GPU 메모리(캐시)에 저장해둡니다.
*   **Step 3:** "간다"를 생성할 때, 앞의 "나는", "학교에"는 계산 안 합니다. 캐시에서 꺼내오고, 이번에 새로 계산한 "간다"의 $K, V$만 캐시에 추가합니다.

---

### 2. Speculative Decoding에서 KV 캐시가 중요한 이유

Speculative Decoding(추측 디코딩)의 핵심은 **"작은 모델이 쓴 초안(Draft)을 큰 모델이 한방에 검사(Verify)한다"**는 것입니다. 여기서 KV 캐시가 결정적인 역할을 합니다.

상황을 가정해봅시다.
*   **작은 모델(Drafter):** "A B C"라는 토큰 3개를 순식간에 제안했습니다.
*   **큰 모델(Verifier):** 이제 "A B C"가 맞는지 확인해야 합니다.

#### 이때 "병렬 검증" 단계에서 일어나는 일

큰 모델은 "A B C"를 하나씩 보는 게 아니라, **입력으로 `[이전 문맥 + A, B, C]`를 통째로 넣고 한 번만 돌립니다(One Forward Pass).**

1.  **병렬 연산:** 큰 모델은 트랜스포머 특성상 입력된 모든 토큰(A, B, C)의 $Q, K, V$를 동시에 계산할 수 있습니다.
2.  **캐시 업데이트:** 계산 과정에서 A, B, C에 대한 $K, V$ 값이 만들어집니다.
3.  **판정 (Accept/Reject):**
    *   만약 A, B, C가 **모두 정답(Accept)**이라고 판정나면?
    *   우리는 이미 검증 과정에서 A, B, C의 $K, V$를 계산했죠?
    *   이걸 **그대로 KV 캐시에 저장(Update)**해버리면 됩니다.
    *   **결과:** 큰 모델은 A, B, C를 생성하기 위해 따로 생쇼를 할 필요 없이, **검증하면서 만든 부산물(KV 값)을 챙기기만 하면 사실상 생성을 완료**한 셈이 됩니다. 이것이 속도 향상의 비결입니다.

4.  **거절 (Reject) 시:**
    *   만약 A, B는 맞는데 **C가 틀렸다**면?
    *   KV 캐시에서 A, B의 값은 남기고(Commit), **C의 값은 버립니다(Rollback).**
    *   그리고 B 다음의 올바른 토큰을 다시 생성합니다.

---

### 3. 요약

1.  **KV 캐시:** 과거 토큰들을 다시 연산하지 않기 위해, Attention 연산의 중간 결과물인 **Key와 Value 벡터를 메모리에 저장해두는 것**입니다.
2.  **Speculative Decoding에서의 역할:**
    *   큰 모델이 드래프트 토큰들을 검증할 때, **어차피 Attention 연산을 수행**해야 합니다.
    *   이때 계산된 $K, V$ 값들을 버리지 않고 **캐시에 저장**해두면, 드래프트가 '정답'으로 판명 났을 때 **"이미 생성을 마친 상태"**가 됩니다.
    *   즉, **"검증(Verification) 비용 = 생성(Generation) 비용"**으로 퉁칠 수 있게 해주는 핵심 메커니즘이 바로 KV 캐시의 재사용입니다.

---
KV 캐시를 사용하지 않았을 경우, 원래의 어텐션 스코어 계산 과정을 알려줘

---
네, **KV 캐시가 없는 '순정' 상태**의 디코더 작동 방식을 설명해 드릴게요.
가장 큰 차이점은 **"입력을 넣어주는 방식"**과 **"계산의 범위"**입니다.

KV 캐시가 없으면, 모델은 금붕어 기억력과 같아서 **지금까지 쓴 문장을 매번 처음부터 다시 읽어야 합니다.**

---

### 기본 원칙: "전체 입력, 전체 재계산"

KV 캐시가 없으면, 다음 토큰을 생성하기 위해 **[지금까지 생성된 모든 토큰]**을 한꺼번에 입력으로 넣습니다.

---

### Step 1. 첫 시작 ("나는" 입력)

*   **입력($X$):** `["나는"]` (길이: 1)
*   **연산:**
    1.  `"나는"`의 $Q_1, K_1, V_1$을 계산합니다.
    2.  **어텐션 연산:** $Q_1 \times K_1^T$ (자기 자신과 내적)
    3.  결과: $1 \times 1$ 크기의 어텐션 맵 생성.
*   **출력:** 다음 토큰 **`"학교에"`** 예측 성공.

---

### Step 2. 두 번째 스텝 (중요: 여기서부터 낭비 시작)

우리는 이제 `"학교에"` 다음 단어를 알고 싶습니다. 하지만 캐시(기억)가 없으므로, 모델에게 **앞부분 문맥을 통째로 다시** 넣어줘야 합니다.

*   **입력($X$):** `["나는", "학교에"]` (길이: 2)
    *   *아까 썼던 "나는"을 또 넣습니다.*
*   **연산 (병렬로 동시에 수행됨):**
    1.  `"나는"`의 $Q_1, K_1, V_1$을 **다시 계산합니다.** (낭비!)
    2.  `"학교에"`의 $Q_2, K_2, V_2$를 **새로 계산합니다.**
    3.  **어텐션 연산 (행렬 곱):**
        *   **Row 1 ("나는"의 관점):** `"나는"` vs `["나는", "학교에"]` (마스킹 때문에 실제로는 `"나는"`만 봄)
        *   **Row 2 ("학교에"의 관점):** `"학교에"` vs `["나는", "학교에"]`
            *   여기서 $Q_2$ ("학교에")와 $K_1$ ("나는"), $K_2$ ("학교에")의 내적이 발생합니다.
    4.  결과: $2 \times 2$ 크기의 어텐션 맵 생성.
*   **출력:** 마지막 위치("학교에")에서 다음 토큰 **`"간다"`** 예측 성공.

---

### Step 3. 세 번째 스텝 (낭비 심화)

이제 `"간다"` 다음을 알고 싶습니다. 또 처음부터 다 넣습니다.

*   **입력($X$):** `["나는", "학교에", "간다"]` (길이: 3)
*   **연산 (모두 다시 계산):**
    1.  `"나는"`의 $Q_1, K_1, V_1$ **또 다시 계산.** (3번째 반복)
    2.  `"학교에"`의 $Q_2, K_2, V_2$ **또 다시 계산.** (2번째 반복)
    3.  `"간다"`의 $Q_3, K_3, V_3$ **새로 계산.**
    4.  **어텐션 연산:** $3 \times 3$ 행렬 연산 수행.
        *   **Row 3 ("간다"의 관점):** $Q_3$ ("간다")가 $K_1, K_2, K_3$ ("나는", "학교에", "간다") 모두와 내적을 수행하여 점수를 매김.
*   **출력:** 마지막 위치에서 다음 토큰 **`"."`** 예측.

---

### 핵심 비교: 캐시가 있고 없고의 차이

질문하신 내용인 **"원래 어텐션 스코어 계산 과정"**은 다음과 같습니다.

1.  **입력:** 매 스텝마다 문장이 길어짐 (`T=1`, `T=2`, `T=3`...)
2.  **프로젝션($W_q, W_k, W_v$ 곱하기):** 매 스텝마다 입력된 **모든 토큰**에 대해 $Q, K, V$ 벡터를 다시 만듭니다.
    *   Step 100이 되면, 앞의 99개 단어에 대해 $K, V$를 만드는 연산을 100번째 다시 수행하는 셈입니다.
3.  **어텐션 맵($Q \times K^T$):** 매 스텝마다 거대한 **정사각 행렬($N \times N$)**을 통째로 다시 그립니다.
    *   물론 우리는 "마지막 줄(마지막 토큰의 예측)"만 필요하지만, 행렬 연산 특성상 전체를 다 돌리게 됩니다. (Masking을 씌우더라도 연산 자체는 들어감)

**요약하자면:**
KV 캐시가 없으면, 책을 읽을 때 **다음 단어 하나를 읽기 위해 매번 첫 페이지 첫 줄부터 다시 소리 내어 읽어서 현재 위치까지 오는 것**과 똑같은 과정을 거칩니다.